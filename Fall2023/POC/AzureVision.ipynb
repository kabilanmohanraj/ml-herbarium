{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f17ba8c",
   "metadata": {},
   "source": [
    "# Azure Vision Implementaion - Dima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88c732",
   "metadata": {},
   "source": [
    "## Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18180f",
   "metadata": {},
   "source": [
    "On a high level this notebook utilizes Azure AI Document Intelligence Studio to extract text from a set of Herbarium specimens, extracted text is then fed into OpenAI's GPT-4 Turbo model to extract relevant information in DARWIN JSON format.\n",
    "\n",
    "The notebook has several different components: \n",
    "1. A resizing function that takes an input of a folder of images and resizes all those images to a size smaller than 4mb-\n",
    "    Azure Vision AI requires input images to be 4mb or smaller, run the resizing function as needed.\n",
    "2. Making a UI- running this code creates a friendly UI in the notebook itself. User is able to upload a Herbarium specimen image, click submit, and in 20 seconds obtain extracted results in DARWIN JSON format.\n",
    "3. Saving Results as a .txt- this code takes an input of a folder of Herbarium images, runs them through Azure Vision OCR to extract all text, then all the extracted text is ran through GPT-4 Turbo to extract just the content that is DARWIN JSON relevant format. Each result is then saved in an output folder in .txt format. \n",
    "4. Running model on Cyrillic and Chinese images- same as the previous code however tailored to Cyrillic and Chinese images through custom prompting. Results saved as a .txt file as well.\n",
    "5. Saving Results as a PDF- same as the previous code that saves as a result as a .txt, however offers more insights. By saving as a pdf you are able to view the original image, the image as processed by Azure AI and how Azure AI identifies sections with text, and all text identified by Azure with confidence for each term all in one pdf file. Note: The text is processed by GPT-4 Turbo however that output is not saved in the pdf, you can however see that output in this notebook as an example. \n",
    "6. Experimenting with Cropping Images- This code was created to see if results code be improved by cropping out all parts of an input image except the sections with identified text. Current results produced by the cropping process are worse compared to not cropping, indicating how the current cropping function is too aggressive and needs to be further worked on. \n",
    "\n",
    "\n",
    "Note: Code section 2-6 all require an Azure and OpenAI API key. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1c7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azure-ai-formrecognizer --pre\n",
    "#!pip install opencv-python-headless matplotlib\n",
    "#!pip install matplotlib pillow\n",
    "#!pip install ipywidgets\n",
    "#!pip install shapely\n",
    "#!pip install openai\n",
    "#!pip install reportlab\n",
    "#!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773366f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "\n",
    "# External libraries for image processing and visualization\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Azure AI services\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "# PDF generation library\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "# Gradio for creating interfaces\n",
    "import gradio as gr\n",
    "\n",
    "# Other libraries\n",
    "import numpy as np\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179830cd",
   "metadata": {},
   "source": [
    "# 1. Resizing images to smaller size for API to accept them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f398fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(input_path, output_path, max_size_mb, quality=85):\n",
    "    \"\"\"\n",
    "    Resize the image found at input_path and save it to output_path.\n",
    "    The image is resized to be under max_size_mb megabytes.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    with Image.open(input_path) as img:\n",
    "        # Calculate target size to maintain aspect ratio\n",
    "        ratio = img.width / img.height\n",
    "        target_width = int((max_size_mb * 1024 * 1024 * ratio) ** 0.5)\n",
    "        target_height = int(target_width / ratio)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Save the resized image\n",
    "        resized_img.save(output_path, quality=quality)\n",
    "\n",
    "# change input/ output folders as needed        \n",
    "input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/'\n",
    "output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    output_file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "    # Check if the file is an image\n",
    "    if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        resize_image(file_path, output_file_path, max_size_mb=4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f69d7b",
   "metadata": {},
   "source": [
    "# 2. Making a UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccbd17e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://5b5e36d606e49cc031.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5b5e36d606e49cc031.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. Note: make sure that each output has a 'country' field. If you do not find an explicit country, make your best guess at the country using the context of the other text.\\n{text}\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        return response.choices[0].message['content'] if response.choices else \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def analyze_read(image_stream):\n",
    "    try:\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect the content from the document\n",
    "        document_content = result.content\n",
    "        extracted_info = extract_info(document_content)\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def model_function(image):\n",
    "    # Convert the NumPy array to a PIL Image object\n",
    "    image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "\n",
    "    # Convert the uploaded image to a byte stream\n",
    "    image_bytes = io.BytesIO()\n",
    "    image.save(image_bytes, format='JPEG')  # Using 'JPEG' as the format\n",
    "    image_bytes = image_bytes.getvalue()\n",
    "\n",
    "    output_text = analyze_read(image_bytes)\n",
    "    return output_text\n",
    "\n",
    "iface = gr.Interface(fn=model_function, inputs=\"image\", outputs=\"text\")\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb317d2a",
   "metadata": {},
   "source": [
    "# 3. Saving Results as a .txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6488535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. Note: make sure that each output has a 'country' field. If you do not find an explicit country, make your best guess at the country using the context of the other text.\\n{text}\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        return response.choices[0].message['content'] if response.choices else \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect the content from the document\n",
    "        document_content = result.content\n",
    "        extracted_info = extract_info(document_content)\n",
    "\n",
    "        # Save the extracted information to a text file\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.txt')))\n",
    "        with open(output_filename, 'w') as text_file:\n",
    "            text_file.write(extracted_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "    output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVisionResults'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 224:  # Stop after processing x images\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95025fa1",
   "metadata": {},
   "source": [
    "# 4. Running Model On Cyrillic & Chinese Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b1e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prompt for cyrillic images\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. NOTE: Parts or the majority of the textmay be in Cyrillic. Take this into consideration. Additionally, there should be a 'country' field, if you cannot determinethe country directly, to your best ability, infer what the country is:\\n{text}\"\n",
    "    \n",
    "    # Prompt for Chinese images\n",
    "    #prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. NOTE: Parts or the majority of the textmay be in Chinese, Take this into consideration. Additionally, there should be a 'country' field, if you cannot determinethe country directly, to your best ability, infer what the country is. Lastly, the more info the better, output chinese character as appropriate:\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        return response.choices[0].message['content'] if response.choices else \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect the content from the document\n",
    "        document_content = result.content\n",
    "        extracted_info = extract_info(document_content)\n",
    "        \n",
    "        #print(extracted_info)\n",
    "\n",
    "\n",
    "        # Save the extracted information to a text file\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.txt')))\n",
    "        with open(output_filename, 'w') as text_file:\n",
    "            text_file.write(extracted_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/CyrillicImages'\n",
    "    #output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/CyrillicResults'\n",
    "    input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/ChineseImages'\n",
    "    output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/ChineseResults'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 35:  # Stop after processing x images\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef5e22",
   "metadata": {},
   "source": [
    "# 5. Saving Results as a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad5cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"00592128\",\n",
      "  \"scientificName\": \"Atriplex patula L. var. hastata Gray\",\n",
      "  \"recordedBy\": \"Kate Furbish\",\n",
      "  \"country\": \"USA\",\n",
      "  \"stateProvince\": \"Maine\",\n",
      "  \"county\": \"Washington\",\n",
      "  \"locality\": \"North Lubec\",\n",
      "  \"eventDate\": \"1902-09-13\",\n",
      "  \"institutionCode\": \"Harvard University Herbaria\",\n",
      "  \"catalogNumber\": \"00592128\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"rightsHolder\": \"President and Fellows of Harvard College\"\n",
      "}\n",
      "```\n",
      "Below is the JSON representation of the relevant Darwin Core fields extracted from the provided text. The Darwin Core standard has many possible fields, but I've put together the ones that can be inferred from the text you've supplied:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"SYNTYPE\",\n",
      "  \"scientificName\": \"Glaux maritima var. obtusifolia Fernald\",\n",
      "  \"acceptedNameUsage\": \"Glaux maritima L.\",\n",
      "  \"verbatimEventDate\": \"July 24, 1902\",\n",
      "  \"recordNumber\": \"00936578, 00936579\",\n",
      "  \"locality\": \"Brackish marsh, St. Andrews, BATHURST, GLOUCESTER COUNTY\",\n",
      "  \"stateProvince\": \"New Brunswick\",\n",
      "  \"country\": \"Canada\",\n",
      "  \"identifiedBy\": \"Walter T. Kittredge\",\n",
      "  \"dateIdentified\": \"2019\",\n",
      "  \"recordedBy\": \"E. F. Williams, M. L. Fernald\",\n",
      "  \"institutionCode\": \"Harvard University Herbaria\",\n",
      "  \"catalogNumber\": \"Not provided (use recordNumber instead)\",\n",
      "  \"publicationCitation\": \"Fernald, Rhodora 4: 215. 1902\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"occurrenceID\": \"Not provided\",\n",
      "  \"higherGeography\": \"North America\",\n",
      "  \"coordinateUncertaintyInMeters\": \"Not provided\"\n",
      "}\n",
      "```\n",
      "\n",
      "Please note that some fields usually found in Darwin Core records such as `occurrenceID` and an exact `catalogNumber` cannot be reliably derived from the text provided. Additionally, the common name and higher taxonomy are not specified in the text. Where confidence levels were low or information could not be precisely determined from the text (e.g., exact coordinates, altitude), the \"Not provided\" value is used to indicate missing data.\n",
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"3040830\",\n",
      "  \"institutionCode\": \"NATIONAL HERBARIUM UNITED STATES\",\n",
      "  \"collectionCode\": \"PLANTS OF ILLINOIS\",\n",
      "  \"catalogNumber\": \"04385325\",\n",
      "  \"scientificName\": \"Sporobolus clandestinus (Spreng) Hitche\",\n",
      "  \"country\": \"US\",\n",
      "  \"stateProvince\": \"Illinois\",\n",
      "  \"county\": \"Henderson\",\n",
      "  \"locality\": \"Sandy bluff of Mississippi river, north of Oquawka\",\n",
      "  \"eventDate\": \"1941-08-27\",\n",
      "  \"recordedBy\": \"D. E. AND M. S. EYLES\",\n",
      "  \"recordNumber\": \"389\",\n",
      "  \"decimalLatitude\": \"\",\n",
      "  \"decimalLongitude\": \"\",\n",
      "  \"coordinateUncertaintyInMeters\": \"\",\n",
      "  \"identifiedBy\": \"\",\n",
      "  \"dateIdentified\": \"\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"individualCount\": \"\",\n",
      "  \"preparations\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Note**: \n",
      "\n",
      "- The `decimalLatitude` and `decimalLongitude` fields did not contain valid entries in the provided text that could be extracted with confidence. Hence, they were left empty.\n",
      "- Similarly, no data was given for the `coordinateUncertaintyInMeters`, `identifiedBy`, `dateIdentified`, `individualCount`, and `preparations`. These fields must be populated with relevant data if available, or left empty/blanks as shown.\n",
      "- The format of `eventDate` was standardized to an ISO 8601 format (YYYY-MM-DD).\n",
      "- `basisOfRecord` was assumed to be \"PreservedSpecimen\" as the standard for physical specimens. If this is not the case, it should be adjusted accordingly based on the exact nature of the record.\n",
      "\n",
      "Please make sure to validate the specifics (e.g., precise latitude/longitude, identifier's credentials, date of identification, etc.) and fill in any missing or uncertain parts from the original data if necessary.\n",
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"03587160\",\n",
      "  \"institutionCode\": \"Smithsonian Institution\",\n",
      "  \"collectionCode\": \"United States National Herbarium\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"scientificName\": \"Nasturtium palustre (L.) DC.\",\n",
      "  \"acceptedNameUsage\": \"Rorippa islandica (Oeder ex Murray) Borbás\",\n",
      "  \"family\": \"Brassicaceae\",\n",
      "  \"eventDate\": \"1965-05-22\",\n",
      "  \"year\": \"1965\",\n",
      "  \"month\": \"5\",\n",
      "  \"day\": \"22\",\n",
      "  \"country\": \"United States of America\",\n",
      "  \"stateProvince\": \"Nebraska\",\n",
      "  \"county\": \"Cedar Co.\",\n",
      "  \"recordedBy\": \"Fred Clements\",\n",
      "  \"identifiedBy\": \"Ronald L. Stuckey\",\n",
      "  \"catalogNumber\": \"2618\",\n",
      "  \"otherCatalogNumbers\": \"219017\",\n",
      "  \"decimalLatitude\": \"\",\n",
      "  \"decimalLongitude\": \"\",\n",
      "  \"coordinateUncertaintyInMeters\": \"\",\n",
      "  \"verbatimCoordinates\": \"\",\n",
      "  \"geodeticDatum\": \"\",\n",
      "  \"verbatimSRS\": \"\",\n",
      "  \"informationWithheld\": \"\",\n",
      "  \"dataGeneralizations\": \"\",\n",
      "  \"dynamicProperties\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "Note: Coordinates and some other specific details like uncertainty and datum were not included in the provided text, so corresponding fields are left blank. The accepted name usage is assumed as per the identifier's determination. The collector's full name appears to be 'Fred Clements' based on the different notations. The confidence values provided do not affect the Darwin Core formatted data unless it specifically impacts the interpretation of relevant fields.\n",
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"1080232\",\n",
      "  \"institutionCode\": \"University of Minnesota\",\n",
      "  \"collectionCode\": \"Herbarium\",\n",
      "  \"recordedBy\": \"Sharon S. & Arthur O. Tucker\",\n",
      "  \"recordNumber\": \"181466\",\n",
      "  \"year\": \"1993\",\n",
      "  \"scientificName\": \"Blephilia hirsuta (Pursh) Benth.\",\n",
      "  \"locality\": \"Houston Co.\",\n",
      "  \"eventDate\": \"1899-07-11\",\n",
      "  \"country\": \"USA\",\n",
      "  \"stateProvince\": \"Minnesota\",\n",
      "  \"county\": \"Houston\",\n",
      "  \"catalogNumber\": \"181466\",\n",
      "  \"identifiedBy\": \"H.S. Lyon\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "\n",
    "def draw_boxes(image_path, words):\n",
    "    original_image = Image.open(image_path)\n",
    "    annotated_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for word in words:\n",
    "        polygon = word['polygon']\n",
    "        if polygon:\n",
    "            bbox = [(point.x, point.y) for point in polygon]\n",
    "            try:\n",
    "                # Replace special characters that cannot be encoded in 'latin-1'\n",
    "                text_content = word['content'].encode('ascii', 'ignore').decode('ascii')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {word['content']}: {e}\")\n",
    "                text_content = \"Error\"\n",
    "            draw.polygon(bbox, outline=\"red\")\n",
    "            draw.text((bbox[0][0], bbox[0][1]), text_content, fill=\"green\")\n",
    "\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "page_width, page_height = letter \n",
    "\n",
    "# Function to calculate scale to fit the image within page dimensions\n",
    "def calculate_scale(image, max_width, max_height):\n",
    "    scale_w = max_width / image.width\n",
    "    scale_h = max_height / image.height\n",
    "    return min(scale_w, scale_h)\n",
    "\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen:\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        if response.choices:\n",
    "            return response.choices[0].message['content']\n",
    "        else:\n",
    "            return \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "       # Collect words, their polygon data, and confidence\n",
    "        words = []\n",
    "        confidence_text = \"\"\n",
    "        for page in result.pages:\n",
    "            for word in page.words:\n",
    "                words.append({\n",
    "                    'content': word.content,\n",
    "                    'polygon': word.polygon\n",
    "                })\n",
    "                confidence_text += \"'{}' confidence {}\\n\".format(word.content, word.confidence)\n",
    "\n",
    "        document_content = result.content + \"\\n\\nConfidence Metrics:\\n\" + confidence_text\n",
    "        extracted_info = extract_info(document_content)\n",
    "        print(extracted_info)\n",
    "        original_image = Image.open(image_path)\n",
    "        annotated_img = draw_boxes(image_path, words)\n",
    "        \n",
    "\n",
    "        # Set up PDF\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.pdf')))\n",
    "        c = canvas.Canvas(output_filename, pagesize=letter)\n",
    "        width, height = letter  # usually 612 x 792\n",
    "\n",
    "        # Draw original image\n",
    "        if original_image.height <= height:\n",
    "            c.drawImage(image_path, 0, height - original_image.height, width=original_image.width, height=original_image.height, mask='auto')\n",
    "            y_position = height - original_image.height\n",
    "        else:\n",
    "            # Handle large images or add scaling logic here\n",
    "            pass\n",
    "\n",
    "        \n",
    "        # Draw original image\n",
    "        scale = calculate_scale(original_image, page_width, page_height)\n",
    "        img_width, img_height = original_image.width * scale, original_image.height * scale\n",
    "        c.drawImage(image_path, 0, page_height - img_height, width=img_width, height=img_height, mask='auto')\n",
    "        y_position = page_height - img_height\n",
    "\n",
    "        # Draw annotated image\n",
    "        annotated_image_path = '/tmp/annotated_image.png'\n",
    "        annotated_img.save(annotated_image_path)\n",
    "        scale = calculate_scale(annotated_img, page_width, page_height)\n",
    "        annotated_img_width, annotated_img_height = annotated_img.width * scale, annotated_img.height * scale\n",
    "        if y_position - annotated_img_height >= 0:\n",
    "            c.drawImage(annotated_image_path, 0, y_position - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        else:\n",
    "            c.showPage()\n",
    "            c.drawImage(annotated_image_path, 0, page_height - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        \n",
    "\n",
    "        # Add text\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(10, y_position - 15)\n",
    "        textobject.setFont(\"Times-Roman\", 12)\n",
    "\n",
    "        for line in document_content.split('\\n'):\n",
    "            if textobject.getY() - 15 < 0:  # Check if new page is needed for more text\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(10, height - 15)\n",
    "                textobject.setFont(\"Times-Roman\", 12)\n",
    "            textobject.textLine(line)\n",
    "        \n",
    "        c.drawText(textobject)\n",
    "        c.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "    output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/temp'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 5:  # Stop after processing x images\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791a84b",
   "metadata": {},
   "source": [
    "# 6. Experimenting With Cropping Images\n",
    "\n",
    "So far results have been better for images that have not been cropped (cropping function needs improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0998c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"AZURE KEY HERE\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "def draw_boxes(image_path, words):\n",
    "    original_image = Image.open(image_path)\n",
    "    annotated_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for word in words:\n",
    "        polygon = word['polygon']\n",
    "        if polygon:\n",
    "            bbox = [(point.x, point.y) for point in polygon]\n",
    "            try:\n",
    "                # Replace special characters that cannot be encoded in 'latin-1'\n",
    "                text_content = word['content'].encode('ascii', 'ignore').decode('ascii')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {word['content']}: {e}\")\n",
    "                text_content = \"Error\"\n",
    "            draw.polygon(bbox, outline=\"red\")\n",
    "            draw.text((bbox[0][0], bbox[0][1]), text_content, fill=\"green\")\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "page_width, page_height = letter \n",
    "\n",
    "# Function to calculate scale to fit the image within page dimensions\n",
    "def calculate_scale(image, max_width, max_height):\n",
    "    scale_w = max_width / image.width\n",
    "    scale_h = max_height / image.height\n",
    "    return min(scale_w, scale_h)\n",
    "\n",
    "\n",
    "def get_text_density_map(pages):\n",
    "    density_map = {}\n",
    "    for page in pages:\n",
    "        for line in page.lines:\n",
    "            points = line.polygon\n",
    "            if points:\n",
    "                x_center = sum(point.x for point in points) / len(points)\n",
    "                y_center = sum(point.y for point in points) / len(points)\n",
    "                density_map[(x_center, y_center)] = density_map.get((x_center, y_center), 0) + 1\n",
    "    return density_map\n",
    "\n",
    "def find_highest_density_area(density_map):\n",
    "    # This function will find the center of the area with the highest text density\n",
    "    # For simplicity, this example just returns the center with the highest count\n",
    "    # In a real scenario, you might want to consider a more sophisticated method\n",
    "    # that takes into account the size and proximity of the high-density areas\n",
    "    return max(density_map, key=density_map.get)\n",
    "\n",
    "def crop_image_to_text(image_path, density_center, crop_size):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Calculate the coordinates for the crop\n",
    "        left = max(density_center[0] - crop_size[0] // 2, 0)\n",
    "        upper = max(density_center[1] - crop_size[1] // 2, 0)\n",
    "        right = min(density_center[0] + crop_size[0] // 2, img.width)\n",
    "        lower = min(density_center[1] + crop_size[1] // 2, img.height)\n",
    "\n",
    "        # Debug output\n",
    "        print(f\"Cropping coordinates: left={left}, upper={upper}, right={right}, lower={lower}\")\n",
    "\n",
    "        # Perform the crop\n",
    "        cropped_img = img.crop((left, upper, right, lower))\n",
    "        return cropped_img\n",
    "    \n",
    "    \n",
    "def get_text_bounding_boxes(pages):\n",
    "    bounding_boxes = []\n",
    "    for page in pages:\n",
    "        for line in page.lines:\n",
    "            if line.polygon:\n",
    "                box = [(point.x, point.y) for point in line.polygon]\n",
    "                bounding_boxes.append(box)\n",
    "    return bounding_boxes\n",
    "\n",
    "def combine_text_regions(image_path, bounding_boxes):\n",
    "    original_image = Image.open(image_path)\n",
    "\n",
    "    # Calculate the combined bounding box\n",
    "    min_x = min(min(box, key=lambda x: x[0])[0] for box in bounding_boxes)\n",
    "    min_y = min(min(box, key=lambda x: x[1])[1] for box in bounding_boxes)\n",
    "    max_x = max(max(box, key=lambda x: x[0])[0] for box in bounding_boxes)\n",
    "    max_y = max(max(box, key=lambda x: x[1])[1] for box in bounding_boxes)\n",
    "\n",
    "    # Create a new blank image with integer dimensions\n",
    "    combined_image = Image.new('RGB', (int(max_x - min_x), int(max_y - min_y)), (255, 255, 255))\n",
    "    \n",
    "    for box in bounding_boxes:\n",
    "        cropped_region = original_image.crop((min(box, key=lambda x: x[0])[0], \n",
    "                                              min(box, key=lambda x: x[1])[1], \n",
    "                                              max(box, key=lambda x: x[0])[0], \n",
    "                                              max(box, key=lambda x: x[1])[1]))\n",
    "        # Paste the cropped region at integer coordinates\n",
    "        combined_image.paste(cropped_region, (int(box[0][0] - min_x), int(box[0][1] - min_y)))\n",
    "\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "def parse_document_content(content):\n",
    "    openai.api_key = 'your-api-key'\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-4\",\n",
    "            prompt=f\"Extract specific information from the following text: {content}\\n\\nSpecies Name: \",\n",
    "            max_tokens=100\n",
    "            # Add additional parameters as needed\n",
    "        )\n",
    "        parsed_data = response.choices[0].text.strip()\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "####################################################\n",
    "\n",
    "def analyze_text_density_and_crop(image_path):\n",
    "    document_analysis_client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "    \n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_stream = f.read()\n",
    "\n",
    "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-read\", image_stream)\n",
    "    result = poller.result()\n",
    "\n",
    "    # Get bounding boxes of text regions\n",
    "    bounding_boxes = get_text_bounding_boxes(result.pages)\n",
    "\n",
    "    # Combine the text regions into one image\n",
    "    combined_image = combine_text_regions(image_path, bounding_boxes)\n",
    "\n",
    "    # Save the combined image temporarily and return its path\n",
    "    combined_image_path = '/tmp/combined_image.png'\n",
    "    combined_image.save(combined_image_path)\n",
    "    return combined_image_path\n",
    "\n",
    "\n",
    "def analyze_read(image_path, output_path, show_first_output=False):\n",
    "    combined_image_path = analyze_text_density_and_crop(image_path)\n",
    "\n",
    "    try:\n",
    "        # Process the combined image with Azure Form Recognizer\n",
    "        with open(combined_image_path, \"rb\") as f:\n",
    "            combined_image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", combined_image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect words, their polygon data, and confidence\n",
    "        words = []\n",
    "        for page in result.pages:\n",
    "            for word in page.words:\n",
    "                words.append({\n",
    "                    'content': word.content,\n",
    "                    'polygon': word.polygon\n",
    "                })\n",
    "\n",
    "        # Prepare annotated image\n",
    "        annotated_img = draw_boxes(combined_image_path, words)\n",
    "\n",
    "        # Set up PDF\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.pdf')))\n",
    "        c = canvas.Canvas(output_filename, pagesize=letter)\n",
    "\n",
    "        # Draw original image\n",
    "        original_image = Image.open(image_path)\n",
    "        scale = calculate_scale(original_image, page_width, page_height)\n",
    "        img_width, img_height = original_image.width * scale, original_image.height * scale\n",
    "        c.drawImage(image_path, 0, page_height - img_height, width=img_width, height=img_height, mask='auto')\n",
    "        y_position = page_height - img_height\n",
    "\n",
    "        # Draw annotated combined image\n",
    "        annotated_image_path = '/tmp/annotated_image.png'\n",
    "        annotated_img.save(annotated_image_path)\n",
    "        scale = calculate_scale(annotated_img, page_width, page_height)\n",
    "        annotated_img_width, annotated_img_height = annotated_img.width * scale, annotated_img.height * scale\n",
    "        if y_position - annotated_img_height >= 0:\n",
    "            c.drawImage(annotated_image_path, 0, y_position - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        else:\n",
    "            c.showPage()  # Start a new page if not enough space\n",
    "            c.drawImage(annotated_image_path, 0, page_height - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "\n",
    "        # Add text\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(10, y_position - 15)\n",
    "        textobject.setFont(\"Times-Roman\", 12)\n",
    "\n",
    "        document_content = '\\n'.join([word['content'] for word in words])\n",
    "        for line in document_content.split('\\n'):\n",
    "            if textobject.getY() - 15 < 0:  # Check if new page is needed for more text\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(10, page_height - 15)\n",
    "                textobject.setFont(\"Times-Roman\", 12)\n",
    "            textobject.textLine(line)\n",
    "\n",
    "        c.drawText(textobject)\n",
    "        c.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {combined_image_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "    output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder, show_first_output=not first_output_shown)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 1:  # Stop after processing 5 images\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b604fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
