{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee93c039-d72d-475a-a752-0c211f7ed81c",
   "metadata": {},
   "source": [
    "## [Introduction] Scoring Mechanism for Document Classification\n",
    "\n",
    "In this script, we perform document classification to categorize images into \"handwritten\" and \"typed\" documents. The scoring is done in multiple stages:\n",
    "\n",
    "1. **Object Detection using DETR**: The initial step uses the DETR model to identify bounding boxes of labels in the images.\n",
    "\n",
    "2. **Text Detection using CRAFT**: CRAFT is then used to further refine the bounding areas, focusing on text regions.\n",
    "\n",
    "3. **Feature Extraction using TrOCR**: Each of the refined bounding areas is processed using TrOCR to extract image features.\n",
    "\n",
    "4. **Classification**: A pre-trained custom classifier (decoder) takes these features to score the likelihood that the document is handwritten or typed.\n",
    "\n",
    "Step-by-step explanation of the scoring mechanism:\n",
    "\n",
    "1. **Initialize Dictionaries**: \n",
    "    - `score_sum_dict` keeps track of the cumulative scores for each type (handwritten and typed) for each document. \n",
    "    - `score_len_dict` keeps track of the number of bounding boxes considered for each type for each document.\n",
    "\n",
    "2. **Processing Loop**:\n",
    "    - For each bounding box in each document, we extract features using TrOCR.\n",
    "    - These features are then input to the classifier which outputs a sigmoid activated score between 0 and 1. \n",
    "    - If the score rounds to 0, it is considered \"handwritten,\" and if it rounds to 1, it is considered \"typed.\"\n",
    "    - The score is added to the corresponding category (handwritten or typed) in `score_sum_dict`, and the count is incremented in `score_len_dict`.\n",
    "\n",
    "3. **Averaging Scores**:\n",
    "    - Once all bounding boxes for all documents are processed, the script calculates the average score for each category (handwritten and typed) for each document.\n",
    "\n",
    "4. **Final Classification**:\n",
    "    - The average scores are compared, and the document is classified into the category with the higher average score. The images are then moved to their respective folders (\"handwritten\" or \"typed\").\n",
    "\n",
    "This scoring mechanism allows us to evaluate the likelihood that a document is handwritten or typed based on multiple bounding boxes, providing a more robust classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e9f4e-7dab-4794-8657-1544855c2bc0",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f71eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from craft_text_detector import Craft\n",
    "\n",
    "from transformers import (TrOCRProcessor, \n",
    "                        VisionEncoderDecoderModel)\n",
    "\n",
    "# add parent directory to path so that we can import our python scripts from all subdirectories\n",
    "cwd_prefix = \"/projectnb/sparkgrp/ml-herbarium-grp/summer2023/kabilanm/ml-herbarium/trocr/evaluation-dataset/handwritten-typed-text-classification/\"\n",
    "import sys\n",
    "sys.path.append(cwd_prefix)\n",
    "\n",
    "import detr\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba4d4d-8451-4e46-b1ca-f982a87f4ec8",
   "metadata": {},
   "source": [
    "## Initialize DETR and CRAFT-Related Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72b051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_inputdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/'\n",
    "detr_outputdir = cwd_prefix+'data/Doc_Classification/intermediate_files/'\n",
    "output_dir_craft = cwd_prefix+'data/Doc_Classification/input/'\n",
    "cache_dir = cwd_prefix+'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b0850d-eaf4-4053-b2f0-6581613c7c94",
   "metadata": {},
   "source": [
    "## DETR Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aebd70-863e-4949-ac88-168500783932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the DETR model for inference (adopted from Freddie (https://github.com/freddiev4/comp-vision-scripts/blob/main/object-detection/detr.py))\n",
    "detr_model = 'spark-ds549/detr-label-detection'\n",
    "# The DETR model returns the bounding boxes of the lables indentified from the images\n",
    "label_bboxes = detr.run(detr_inputdir, detr_outputdir, detr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69458a-3a0b-4327-bf59-655bfd1271d7",
   "metadata": {},
   "source": [
    "## Initialize CRAFT Model and Get Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ef3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/251 [00:01<02:14,  1.85it/s]/projectnb/sparkgrp/kabilanm/.conda/envs/trocr_env/lib/python3.9/site-packages/craft_text_detector/craft_utils.py:415: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  polys = np.array(polys)\n",
      "/projectnb/sparkgrp/kabilanm/.conda/envs/trocr_env/lib/python3.9/site-packages/craft_text_detector/predict.py:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  polys_as_ratio = np.array(polys_as_ratio)\n",
      "100%|██████████| 251/251 [04:00<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize the CRAFT model\n",
    "craft = Craft(output_dir = output_dir_craft, \n",
    "              export_extra = False, \n",
    "              text_threshold = .7, \n",
    "              link_threshold = .4, \n",
    "              crop_type=\"poly\", \n",
    "              low_text = .3, \n",
    "              cuda = True)\n",
    "\n",
    "# CRAFT on images to get bounding boxes\n",
    "images = []\n",
    "corrupted_images = []\n",
    "no_segmentations = []\n",
    "boxes = {}\n",
    "count= 0\n",
    "img_name = []\n",
    "box = []\n",
    "file_types = (\".jpg\", \".jpeg\",\".png\")\n",
    "    \n",
    "for filename in tqdm(sorted(os.listdir(detr_outputdir))):\n",
    "    if filename.endswith(file_types):\n",
    "        image = detr_outputdir+filename\n",
    "        try:\n",
    "            img = Image.open(image) \n",
    "            img.verify() # Check that the image is valid\n",
    "            bounding_areas = craft.detect_text(image)\n",
    "            if len(bounding_areas['boxes']): #check that a segmentation was found\n",
    "                images.append(image)\n",
    "                boxes[image] = bounding_areas['boxes']\n",
    "                \n",
    "            else:\n",
    "                no_segmentations.append(image)\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            corrupted_images.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcdf2a-cd5e-4d69-99c1-e97ddbf4fe4e",
   "metadata": {},
   "source": [
    "## Initialize Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd69b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f37c26-ec79-4453-8061-cf42dcf62dbf",
   "metadata": {},
   "source": [
    "## Initialize Processor and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669355fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf432d5760804a358a3026b67b413ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca8b6ad716b441aa5e130d33526d96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab36eebd5154f87b938263bf04d59d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88b87a69a1b421097684c163c68592f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9953ed549f4e8b96e0eba80296d94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6b2d11b2f24d56a0363ac61ba3106b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422fdb1860044a78be5173e2195c9136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0b9004fd884bd5901610f3037ed853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-stage1 and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f1cea20ab142218d6003da3c8ade1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model and processor\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-stage1', cache_dir=cache_dir)\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-stage1', cache_dir=cache_dir)\n",
    "\n",
    "# Freeze TrOCR layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define our custom classifier (also decoder)\n",
    "classifier = nn.Sequential(\n",
    "    \n",
    "    nn.Conv2d(1, 16, kernel_size=1, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(16, 32, kernel_size=1, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(32, 32, kernel_size=1, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * (577 // 8) * (1024 // 8), 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(256, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c3bb4-a141-4091-b010-46fa4b1ff392",
   "metadata": {},
   "source": [
    "## Load Pretrained Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb68f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = torch.nn.DataParallel(classifier, [0]) # list(range(torch.cuda.device_count()))\n",
    "classifier.load_state_dict(torch.load(cwd_prefix+\"model/TrOCR_L_enc_feature_extraction_w_classifier_retrained.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850d0660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    (10): Linear(in_features=294912, out_features=512, bias=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "    (15): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move Models to Device\n",
    "model.to(device)\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a31375-25b9-461b-9cd5-2395c7aaaaf1",
   "metadata": {},
   "source": [
    "## Initialize Scoring Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ec18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sum_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_confidence, typed_confidence)\n",
    "score_len_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_count, typed_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8594642-5029-4248-8df7-45ec12d5cc52",
   "metadata": {},
   "source": [
    "## Process Each Image and Compute Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5238bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in os.listdir(output_dir_craft):\n",
    "    for file in os.listdir(os.path.join(output_dir_craft, dir_)):\n",
    "        \n",
    "        key = dir_.split(\"_\")[0]\n",
    "        \n",
    "        img = Image.open(output_dir_craft+dir_+\"/\"+file)\n",
    "        \n",
    "        pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        encoder_outputs = model.encoder(pixel_values)\n",
    "        \n",
    "        image_representation = encoder_outputs.last_hidden_state\n",
    "\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            classifier_output = classifier(image_representation.unsqueeze(1))\n",
    "            \n",
    "            pred_confidence = torch.sigmoid(classifier_output)\n",
    "            predicted = torch.round(pred_confidence)\n",
    "            \n",
    "            if(predicted == 0):\n",
    "                score_sum_dict[key][0] += 1-pred_confidence\n",
    "                score_len_dict[key][0] += 1\n",
    "            if(predicted == 1):\n",
    "                score_sum_dict[key][1] += pred_confidence\n",
    "                score_len_dict[key][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adbd732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sum_dict = dict(score_sum_dict)\n",
    "score_len_dict = dict(score_len_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b59a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_avg_dict = defaultdict(lambda: [0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1e9d4-e94f-4692-8e81-3982255785f7",
   "metadata": {},
   "source": [
    "## Final Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a8e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating and computing final scores\n",
    "hw_score, typed_score = 0, 0\n",
    "\n",
    "for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):\n",
    "    if(len_[1][0] == 0):\n",
    "        hw_score = 0\n",
    "    elif(len_[1][1] == 0):\n",
    "        typed_score = 0\n",
    "    else:\n",
    "        hw_score = sum_[1][0]/len_[1][0]\n",
    "        typed_score = sum_[1][1]/len_[1][1]\n",
    "    score_avg_dict[sum_[0]] = [hw_score, typed_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_avg_dict = dict(score_avg_dict)\n",
    "score_avg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa069a1-e172-43ae-ab58-7734c859d42e",
   "metadata": {},
   "source": [
    "## Classify Files Based on Scores\n",
    "\n",
    "Here, we copy the images to the respective directories based on the average confidence scores computed for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835cf18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"data/Doc_Classification/output/\"\n",
    "\n",
    "for file_name, avg_scores in score_avg_dict.items():\n",
    "    # print(detr_inputdir+file_name+\".jpg\", scores)\n",
    "    # print(file_name, avg_scores)\n",
    "    \n",
    "    source_file = detr_inputdir+file_name+\".jpg\"\n",
    "    \n",
    "    # Copy the file using shutil.copy2 to the corresponding directory\n",
    "    # based on the average prediction score\n",
    "    if(avg_scores[0] >= avg_scores[1]):\n",
    "        # print(\"handwritten\")\n",
    "        shutil.copy2(source_file, os.path.join(output_dir, \"handwritten\"))\n",
    "        \n",
    "    # add some bias here\n",
    "    if(avg_scores[0] < avg_scores[1]):\n",
    "        # print(\"typed\")\n",
    "        shutil.copy2(source_file, os.path.join(output_dir, \"typed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed59759-b3fc-422a-813c-059afa349ed5",
   "metadata": {},
   "source": [
    "## Count Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1b3f954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/Doc_Classification/output/handwritten/ | wc -l\n",
    "! ls ../data/Doc_Classification/output/typed/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32447b5-2d9c-4d6b-99fe-5d0cbea40423",
   "metadata": {},
   "source": [
    "## Cleanup (Optional) but recommended before every new run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c9ed51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf ../data/Doc_Classification/output/handwritten/*\n",
    "# ! rm -rf ../data/Doc_Classification/output/typed/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ce118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
